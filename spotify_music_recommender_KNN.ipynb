{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant imports and initialization code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from numpy import savetxt\n",
    "\n",
    "seed = 25\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reads in our data from our Spotify track dataset. This dataset has a large amount of data that's captured per track.\n",
    "\n",
    "Let's read in our other dataset that contains user created playlist with songs with them. This will be crucial to generating a desired y label that we will want our neural network to train on.\n",
    "\n",
    "You'll need to download the spotify song dataset and the spotify recommendator challenge dataset from https://www.kaggle.com/datasets/zaheenhamidani/ultimate-spotify-tracks-db/ and https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge respectively and put them into the dataset folder in order to run this code.\n",
    "\n",
    "It's not in the git repo since it's too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "original_df = pd.read_csv('./datasets/spotify_songs.csv')\n",
    "original_df.drop_duplicates('track_id', inplace=True)\n",
    "original_df = original_df[['genre','artist_name','track_name','track_id','popularity','acousticness','danceability','duration_ms','energy','instrumentalness','key','liveness','loudness','mode','speechiness','tempo','time_signature','valence']]\n",
    "# we only want a certain number of columns. Reducing unneeded features will improve performance.\n",
    "\n",
    "\n",
    "playlist_df = pd.read_json('./datasets/challenge_set.json')\n",
    "playlist_series = playlist_df['playlists']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code before does not output any information about the genre or track id. This is because in the dataset they are string values.\n",
    "\n",
    "This code section rearranges the dataset to be more compatible with machine learning. One aspect of this is converting unique string values into a numeric equivalent. The Tensorflow normalization layer will handle proper normalization after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4.000000e+03</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.449750</td>\n",
       "      <td>683.860750</td>\n",
       "      <td>5574.526250</td>\n",
       "      <td>5999.500000</td>\n",
       "      <td>46.171250</td>\n",
       "      <td>0.221302</td>\n",
       "      <td>0.558485</td>\n",
       "      <td>2.257594e+05</td>\n",
       "      <td>0.671749</td>\n",
       "      <td>0.033769</td>\n",
       "      <td>5.421250</td>\n",
       "      <td>0.197724</td>\n",
       "      <td>-6.972861</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>0.072259</td>\n",
       "      <td>123.099086</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.487848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497531</td>\n",
       "      <td>441.645122</td>\n",
       "      <td>1342.866369</td>\n",
       "      <td>1154.844867</td>\n",
       "      <td>7.734816</td>\n",
       "      <td>0.264964</td>\n",
       "      <td>0.134234</td>\n",
       "      <td>6.062448e+04</td>\n",
       "      <td>0.211033</td>\n",
       "      <td>0.131651</td>\n",
       "      <td>3.509113</td>\n",
       "      <td>0.161859</td>\n",
       "      <td>3.070843</td>\n",
       "      <td>0.432488</td>\n",
       "      <td>0.079433</td>\n",
       "      <td>30.358727</td>\n",
       "      <td>0.524029</td>\n",
       "      <td>0.225759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>2.671700e+04</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>-25.669000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>47.811000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>323.750000</td>\n",
       "      <td>4680.750000</td>\n",
       "      <td>4999.750000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>1.914582e+05</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>-8.506000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>97.990500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>5653.500000</td>\n",
       "      <td>5999.500000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.558500</td>\n",
       "      <td>2.170065e+05</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>-6.347000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>120.130500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>6639.250000</td>\n",
       "      <td>6999.250000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.649000</td>\n",
       "      <td>2.499168e+05</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>-4.744250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>144.051000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.660250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1696.000000</td>\n",
       "      <td>7634.000000</td>\n",
       "      <td>7999.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>1.355938e+06</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>-0.259000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>213.788000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             genre  artist_name   track_name     track_id   popularity  \\\n",
       "count  4000.000000  4000.000000  4000.000000  4000.000000  4000.000000   \n",
       "mean      3.449750   683.860750  5574.526250  5999.500000    46.171250   \n",
       "std       0.497531   441.645122  1342.866369  1154.844867     7.734816   \n",
       "min       3.000000    40.000000     7.000000  4000.000000     0.000000   \n",
       "25%       3.000000   323.750000  4680.750000  4999.750000    39.000000   \n",
       "50%       3.000000   534.000000  5653.500000  5999.500000    48.000000   \n",
       "75%       4.000000  1018.000000  6639.250000  6999.250000    52.000000   \n",
       "max       4.000000  1696.000000  7634.000000  7999.000000    72.000000   \n",
       "\n",
       "       acousticness  danceability   duration_ms       energy  \\\n",
       "count   4000.000000   4000.000000  4.000000e+03  4000.000000   \n",
       "mean       0.221302      0.558485  2.257594e+05     0.671749   \n",
       "std        0.264964      0.134234  6.062448e+04     0.211033   \n",
       "min        0.000003      0.127000  2.671700e+04     0.018800   \n",
       "25%        0.010675      0.472000  1.914582e+05     0.515000   \n",
       "50%        0.093400      0.558500  2.170065e+05     0.702500   \n",
       "75%        0.376000      0.649000  2.499168e+05     0.848000   \n",
       "max        0.974000      0.936000  1.355938e+06     0.998000   \n",
       "\n",
       "       instrumentalness          key     liveness     loudness         mode  \\\n",
       "count       4000.000000  4000.000000  4000.000000  4000.000000  4000.000000   \n",
       "mean           0.033769     5.421250     0.197724    -6.972861     0.249000   \n",
       "std            0.131651     3.509113     0.161859     3.070843     0.432488   \n",
       "min            0.000000     0.000000     0.021400   -25.669000     0.000000   \n",
       "25%            0.000000     2.000000     0.099300    -8.506000     0.000000   \n",
       "50%            0.000010     5.000000     0.132000    -6.347000     0.000000   \n",
       "75%            0.000937     8.000000     0.254000    -4.744250     0.000000   \n",
       "max            0.949000    11.000000     0.994000    -0.259000     1.000000   \n",
       "\n",
       "       speechiness        tempo  time_signature      valence  \n",
       "count  4000.000000  4000.000000     4000.000000  4000.000000  \n",
       "mean      0.072259   123.099086        0.146500     0.487848  \n",
       "std       0.079433    30.358727        0.524029     0.225759  \n",
       "min       0.022400    47.811000        0.000000     0.034200  \n",
       "25%       0.031800    97.990500        0.000000     0.315000  \n",
       "50%       0.041300   120.130500        0.000000     0.476500  \n",
       "75%       0.072400   144.051000        0.000000     0.660250  \n",
       "max       0.918000   213.788000        3.000000     0.975000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_values(col):\n",
    "    unique_items = col.unique().tolist()\n",
    "    items_to_encoded = {x: i for i, x in enumerate(unique_items)}\n",
    "    encoded_to_items = {i: x for i, x in enumerate(unique_items)}\n",
    "    return (items_to_encoded, encoded_to_items)\n",
    "\n",
    "genre_items_to_encoded, genre_encoded_to_items = encode_values(original_df['genre'])\n",
    "original_df['genre'] = original_df['genre'].map(genre_items_to_encoded)\n",
    "\n",
    "artist_name_items_to_encoded, artist_name_encoded_to_items = encode_values(original_df['artist_name'])\n",
    "original_df['artist_name'] = original_df['artist_name'].map(artist_name_items_to_encoded)\n",
    "\n",
    "track_name_items_to_encoded, track_name_encoded_to_items = encode_values(original_df['track_name'])\n",
    "original_df['track_name'] = original_df['track_name'].map(track_name_items_to_encoded)\n",
    "\n",
    "track_id_items_to_encoded, track_id_encoded_to_items = encode_values(original_df['track_id'])\n",
    "original_df['track_id'] = original_df['track_id'].map(track_id_items_to_encoded)\n",
    "\n",
    "key_items_to_encoded, key_encoded_to_items = encode_values(original_df['key'])\n",
    "original_df['key'] = original_df['key'].map(key_items_to_encoded)\n",
    "\n",
    "mode_items_to_encoded, mode_encoded_to_items = encode_values(original_df['mode'])\n",
    "original_df['mode'] = original_df['mode'].map(mode_items_to_encoded)\n",
    "\n",
    "time_signature_items_to_encoded, time_signature_encoded_to_items = encode_values(original_df['time_signature'])\n",
    "original_df['time_signature'] = original_df['time_signature'].map(time_signature_items_to_encoded)\n",
    "\n",
    "df = original_df.copy()\n",
    "df = df[sample_size:sample_size*2]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a pseudo histogram where we capture the frequency in which a particular song is in a playlist with another song. In training, this will generate the y label based on how frequent a song is found with another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "72000\n",
      "72000\n",
      "0\n",
      "(3600, 18)\n",
      "(400, 18)\n",
      "(3600, 3600)\n",
      "(400, 3600)\n"
     ]
    }
   ],
   "source": [
    "train_indices = int(0.9 * df.shape[0])\n",
    "train_df = df.iloc[:train_indices]\n",
    "count = 0\n",
    "\n",
    "# encoded_track_histogram = {int(row['track_id']): set() for index, row in df.iterrows()}\n",
    "# for playlist in playlist_series:\n",
    "#     playlist_tracks = [track['track_uri'].split(':')[2] for track in playlist['tracks']]\n",
    "#     included_tracks = [track for track in playlist_tracks if track in track_id_items_to_encoded]\n",
    "#     encoded_tracks = [track_id_items_to_encoded[track] for track in included_tracks]\n",
    "    \n",
    "#     if (len(encoded_tracks) > 1):\n",
    "#         for x in encoded_tracks:\n",
    "#             for y in encoded_tracks:\n",
    "#                 encoded_track_histogram[x].add(y)\n",
    "#                 encoded_track_histogram[y].add(x)\n",
    "#                 count = count + 2\n",
    "\n",
    "x_data = df.values.astype(np.float32)\n",
    "y_data = []\n",
    "\n",
    "normalized_df = (df - df.mean()) / df.std()\n",
    "normalized_train_df = normalized_df.iloc[:train_indices]\n",
    "\n",
    "print(len(list(df.iterrows())))\n",
    "print(df.to_numpy().size)\n",
    "print(normalized_df.to_numpy().size)\n",
    "#print(len(encoded_track_histogram))\n",
    "\n",
    "for index, row in normalized_df.iterrows():\n",
    "    probability = np.zeros(train_indices)\n",
    "    distances = {id: np.sqrt(np.sum((np.array(row) - np.array(other_row)) ** 2)) for id, other_row in enumerate(normalized_train_df.to_numpy())}\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    closest = sorted_distances[1:6]\n",
    "\n",
    "    histogram_data = [datapoint[0] for datapoint in closest] #+ [datapoint for datapoint in encoded_track_histogram[index] if datapoint < len(probability)]\n",
    "\n",
    "    probability[histogram_data[0]] = 0.5\n",
    "    probability[histogram_data[1]] = 0.3\n",
    "    probability[histogram_data[2]] = 0.1\n",
    "    probability[histogram_data[3]] = 0.08\n",
    "    probability[histogram_data[4]] = 0.02\n",
    "    # for histogram_datapoint in histogram_data:\n",
    "    #     probability[histogram_datapoint] = probability[histogram_datapoint] + (1 / len(histogram_data))\n",
    "\n",
    "    y_data.append(probability)\n",
    "\n",
    "\n",
    "\n",
    "y_data = np.array(y_data)\n",
    "test = y_data.sum(axis=1)\n",
    "savetxt('test.txt', test)\n",
    "print(count)\n",
    "\n",
    "x_train = x_data[:train_indices]\n",
    "x_test = x_data[train_indices:]\n",
    "y_train = np.array(y_data)[:train_indices]\n",
    "y_test = np.array(y_data)[train_indices:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section builds the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(features, num_classes):\n",
    "    normal_layer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normal_layer.adapt(features)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "      normal_layer,\n",
    "      keras.layers.Dense(2000, activation='relu'),\n",
    "      keras.layers.Dense(2000, activation='relu'),\n",
    "      keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normaliza  (None, 18)                37        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2000)              38000     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2000)              4002000   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3600)              7203600   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11243637 (42.89 MB)\n",
      "Trainable params: 11243600 (42.89 MB)\n",
      "Non-trainable params: 37 (152.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 - 1s - loss: 8.0366 - accuracy: 0.0119 - 1s/epoch - 144ms/step\n",
      "Epoch 2/50\n",
      "8/8 - 1s - loss: 6.7944 - accuracy: 0.0397 - 785ms/epoch - 98ms/step\n",
      "Epoch 3/50\n",
      "8/8 - 1s - loss: 5.2346 - accuracy: 0.1250 - 776ms/epoch - 97ms/step\n",
      "Epoch 4/50\n",
      "8/8 - 1s - loss: 3.7939 - accuracy: 0.2344 - 822ms/epoch - 103ms/step\n",
      "Epoch 5/50\n",
      "8/8 - 1s - loss: 2.9972 - accuracy: 0.3242 - 824ms/epoch - 103ms/step\n",
      "Epoch 6/50\n",
      "8/8 - 1s - loss: 2.7028 - accuracy: 0.3856 - 817ms/epoch - 102ms/step\n",
      "Epoch 7/50\n",
      "8/8 - 1s - loss: 2.5486 - accuracy: 0.4086 - 812ms/epoch - 101ms/step\n",
      "Epoch 8/50\n",
      "8/8 - 1s - loss: 2.4030 - accuracy: 0.4547 - 832ms/epoch - 104ms/step\n",
      "Epoch 9/50\n",
      "8/8 - 1s - loss: 2.2577 - accuracy: 0.4825 - 824ms/epoch - 103ms/step\n",
      "Epoch 10/50\n",
      "8/8 - 1s - loss: 2.1734 - accuracy: 0.5131 - 824ms/epoch - 103ms/step\n",
      "Epoch 11/50\n",
      "8/8 - 1s - loss: 2.1158 - accuracy: 0.5344 - 805ms/epoch - 101ms/step\n",
      "Epoch 12/50\n",
      "8/8 - 1s - loss: 2.0607 - accuracy: 0.5600 - 826ms/epoch - 103ms/step\n",
      "Epoch 13/50\n",
      "8/8 - 1s - loss: 1.9960 - accuracy: 0.5917 - 828ms/epoch - 104ms/step\n",
      "Epoch 14/50\n",
      "8/8 - 1s - loss: 1.9465 - accuracy: 0.6039 - 832ms/epoch - 104ms/step\n",
      "Epoch 15/50\n",
      "8/8 - 1s - loss: 1.9259 - accuracy: 0.6072 - 834ms/epoch - 104ms/step\n",
      "Epoch 16/50\n",
      "8/8 - 1s - loss: 1.8914 - accuracy: 0.6286 - 884ms/epoch - 111ms/step\n",
      "Epoch 17/50\n",
      "8/8 - 1s - loss: 1.8598 - accuracy: 0.6356 - 856ms/epoch - 107ms/step\n",
      "Epoch 18/50\n",
      "8/8 - 1s - loss: 1.8386 - accuracy: 0.6314 - 804ms/epoch - 101ms/step\n",
      "Epoch 19/50\n",
      "8/8 - 1s - loss: 1.8174 - accuracy: 0.6531 - 827ms/epoch - 103ms/step\n",
      "Epoch 20/50\n",
      "8/8 - 1s - loss: 1.7907 - accuracy: 0.6625 - 855ms/epoch - 107ms/step\n",
      "Epoch 21/50\n",
      "8/8 - 1s - loss: 1.7747 - accuracy: 0.6767 - 832ms/epoch - 104ms/step\n",
      "Epoch 22/50\n",
      "8/8 - 1s - loss: 1.7590 - accuracy: 0.6756 - 847ms/epoch - 106ms/step\n",
      "Epoch 23/50\n",
      "8/8 - 1s - loss: 1.7343 - accuracy: 0.6964 - 890ms/epoch - 111ms/step\n",
      "Epoch 24/50\n",
      "8/8 - 1s - loss: 1.7212 - accuracy: 0.6819 - 890ms/epoch - 111ms/step\n",
      "Epoch 25/50\n",
      "8/8 - 1s - loss: 1.6947 - accuracy: 0.7131 - 865ms/epoch - 108ms/step\n",
      "Epoch 26/50\n",
      "8/8 - 1s - loss: 1.6883 - accuracy: 0.7097 - 861ms/epoch - 108ms/step\n",
      "Epoch 27/50\n",
      "8/8 - 1s - loss: 1.6752 - accuracy: 0.7094 - 853ms/epoch - 107ms/step\n",
      "Epoch 28/50\n",
      "8/8 - 1s - loss: 1.6622 - accuracy: 0.7242 - 856ms/epoch - 107ms/step\n",
      "Epoch 29/50\n",
      "8/8 - 1s - loss: 1.6616 - accuracy: 0.7206 - 800ms/epoch - 100ms/step\n",
      "Epoch 30/50\n",
      "8/8 - 1s - loss: 1.6445 - accuracy: 0.7286 - 830ms/epoch - 104ms/step\n",
      "Epoch 31/50\n",
      "8/8 - 1s - loss: 1.6237 - accuracy: 0.7356 - 859ms/epoch - 107ms/step\n",
      "Epoch 32/50\n",
      "8/8 - 1s - loss: 1.6137 - accuracy: 0.7381 - 800ms/epoch - 100ms/step\n",
      "Epoch 33/50\n",
      "8/8 - 1s - loss: 1.6039 - accuracy: 0.7408 - 797ms/epoch - 100ms/step\n",
      "Epoch 34/50\n",
      "8/8 - 1s - loss: 1.5962 - accuracy: 0.7478 - 808ms/epoch - 101ms/step\n",
      "Epoch 35/50\n",
      "8/8 - 1s - loss: 1.5837 - accuracy: 0.7575 - 829ms/epoch - 104ms/step\n",
      "Epoch 36/50\n",
      "8/8 - 1s - loss: 1.5876 - accuracy: 0.7378 - 822ms/epoch - 103ms/step\n",
      "Epoch 37/50\n",
      "8/8 - 1s - loss: 1.5704 - accuracy: 0.7575 - 834ms/epoch - 104ms/step\n",
      "Epoch 38/50\n",
      "8/8 - 1s - loss: 1.5665 - accuracy: 0.7653 - 842ms/epoch - 105ms/step\n",
      "Epoch 39/50\n",
      "8/8 - 1s - loss: 1.5537 - accuracy: 0.7619 - 824ms/epoch - 103ms/step\n",
      "Epoch 40/50\n",
      "8/8 - 1s - loss: 1.5458 - accuracy: 0.7644 - 835ms/epoch - 104ms/step\n",
      "Epoch 41/50\n",
      "8/8 - 1s - loss: 1.5519 - accuracy: 0.7694 - 782ms/epoch - 98ms/step\n",
      "Epoch 42/50\n",
      "8/8 - 1s - loss: 1.5424 - accuracy: 0.7806 - 770ms/epoch - 96ms/step\n",
      "Epoch 43/50\n",
      "8/8 - 1s - loss: 1.5293 - accuracy: 0.7764 - 820ms/epoch - 103ms/step\n",
      "Epoch 44/50\n",
      "8/8 - 1s - loss: 1.5224 - accuracy: 0.7656 - 836ms/epoch - 105ms/step\n",
      "Epoch 45/50\n",
      "8/8 - 1s - loss: 1.5141 - accuracy: 0.7786 - 823ms/epoch - 103ms/step\n",
      "Epoch 46/50\n",
      "8/8 - 1s - loss: 1.5056 - accuracy: 0.7631 - 806ms/epoch - 101ms/step\n",
      "Epoch 47/50\n",
      "8/8 - 1s - loss: 1.5075 - accuracy: 0.7828 - 819ms/epoch - 102ms/step\n",
      "Epoch 48/50\n",
      "8/8 - 1s - loss: 1.5029 - accuracy: 0.7744 - 800ms/epoch - 100ms/step\n",
      "Epoch 49/50\n",
      "8/8 - 1s - loss: 1.5057 - accuracy: 0.7775 - 795ms/epoch - 99ms/step\n",
      "Epoch 50/50\n",
      "8/8 - 1s - loss: 1.5022 - accuracy: 0.7678 - 803ms/epoch - 100ms/step\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.8161 - accuracy: 0.1050\n",
      "Baseline error: 90%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_and_compile_model(x_train, len(x_train))\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=500, verbose=2) #  validation_data=(x_test, y_test)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print('Baseline error: %2.f%%' % (100 - scores[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data point\n",
      "Country\n",
      "Johnny Cash\n",
      "The One on the Right Is on the Left\n",
      "1EYl3ASOxlK4Fk4Q1bhDh4\n",
      "A\n",
      "Major\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "0.18193291\n",
      "Prediction:\n",
      "Country\n",
      "The Doobie Brothers\n",
      "Another Park, Another Sunday - 2006 Remaster\n",
      "5dEOntLHunr3jYzS1XBNmk\n",
      "A\n",
      "Major\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_row_details(data):\n",
    "    print(genre_encoded_to_items[data['genre']])\n",
    "    print(artist_name_encoded_to_items[data['artist_name']])\n",
    "    print(track_name_encoded_to_items[data['track_name']])\n",
    "    print(track_id_encoded_to_items[data['track_id']])\n",
    "    print(key_encoded_to_items[data['key']])\n",
    "    print(mode_encoded_to_items[data['mode']])\n",
    "\n",
    "index = random.randrange(len(df))\n",
    "track_id = '1EYl3ASOxlK4Fk4Q1bhDh4'\n",
    "#data_to_predict = original_df.iloc[index]\n",
    "data_to_predict = original_df.iloc[track_id_items_to_encoded[track_id]]\n",
    "\n",
    "print('Selected data point')\n",
    "print_row_details(data_to_predict)\n",
    "\n",
    "prediction = model.predict(data_to_predict.to_numpy().astype(np.float64))\n",
    "\n",
    "# print(prediction.shape)\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(prediction)\n",
    "# savetxt('data.npy', prediction[0])\n",
    "\n",
    "for row in prediction:\n",
    "    prediction_index = row.argmax()\n",
    "    print(row[prediction_index])\n",
    "    print('Prediction:')\n",
    "    print_row_details(df.iloc[prediction_index])\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
